Compiler/main.py
import sys
import json
from workflow.executor import build_and_run_langgraph_workflow

if __name__ == "__main__":
    if len(sys.argv) < 5:
        print(json.dumps({
            "status": "failed",
            "error": "Usage: python main.py <workflow_json> <initial_input> <thread_id> <mongo_uri>"
        }), file=sys.stderr)
        sys.exit(1)

    workflow_json_str = sys.argv[1]
    initial_input = sys.argv[2]
    thread_id = sys.argv[3]
    mongo_uri = sys.argv[4]

    try:
        workflow_def = json.loads(workflow_json_str)
    except json.JSONDecodeError:
        print(json.dumps({"status": "failed", "error": "Invalid workflow JSON"}), file=sys.stderr)
        sys.exit(1)

    result = build_and_run_langgraph_workflow(workflow_def, initial_input, thread_id, mongo_uri)
    print(json.dumps(result))

Compiler/workflow/executor.py
empty add code

Compiler/utils/mongo.py
from pymongo import MongoClient
from langgraph.checkpoint.mongodb import MongoDBSaver

def get_mongo_checkpointer(mongo_uri):
    client = MongoClient(mongo_uri)
    return client, MongoDBSaver(client)


Compiler/utils/logger.py
from datetime import datetime

def log(message: str, level: str = "info") -> dict:
    return {
        "timestamp": datetime.now().isoformat(),
        "level": level,
        "message": message
    }


Compiler/tools/tools.py
from datetime import datetime
from langchain_core.tools import tool

@tool
def simple_data_processor(data: str) -> str:
    processed = data.upper() + "_PROCESSED"
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "level": "info",
        "message": f"Processed data: {data} -> {processed}"
    }
    return processed, log_entry

tools = [simple_data_processor]


Compiler/llm/llm_init.py
import os
from langchain_openai import ChatOpenAI
from tools.tools import tools

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY is not set.")

llm = ChatOpenAI(model="gpt-4o", temperature=0.7, api_key=OPENAI_API_KEY)
llm_with_tools = llm.bind_tools(tools)


Compiler/agent/state.py
from typing import TypedDict, Annotated, List, Dict, Any
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], lambda a, b: a + b]
    tool_calls: List[Dict]
    tool_response: Any
    current_workflow_data: Dict[str, Any]
    workflow_logs: Annotated[List[Dict[str, Any]], lambda a, b: a + b]


Compiler/agent/nodes.py
from langchain_core.messages import HumanMessage, AIMessage
from tools.tools import tools
from llm.llm_init import llm_with_tools
from utils.logger import log

def call_llm_node(state):
    # Same as before, but use log(...) for logging
    ...

def execute_tools_node(state):
    # Same logic; use tools from tools.py
    ...

def end_workflow_node(state):
    logs = state.get("workflow_logs", [])
    logs.append(log("info", "Workflow reached end node."))
    return {"workflow_logs": logs}


Compiler/agent/conditionals.py
def should_continue(state):
    last_message = state["messages"][-1]
    if "tool_calls" in last_message.additional_kwargs and last_message.additional_kwargs["tool_calls"]:
        return "continue"
    return "end"


solve all error and empty thing 
